---
title: Launching into Machine Learning メモ
date: 2019-08-27T21:51:23+09:00
categories:
- メモ
tags:
- MachineLearning
- Coursera
- ML Study Jam Vol.3
---

各ビデオごとにメモ

## MLの開始について

- このコースは、機械学習の用語や、普及の歴史、モデル作成のためのコードや知識を身につけます
- コースには最適化モジュールがあり、学習問題が用意位されている。最急降下法を使います。

## 実践的なMLについて

- 歴史を振り返って実践のときに参考にできるようします
- グーグルが全プロジェクトに機械学習を取り入れる取り組みについて紹介します

## 教師あり学習

### 教師あり学習

- モデルになにを学習させるか
- これまで発達した理由を理解します
- 教師なし学習は発見するもの
- 教師あり学習はラベルの特徴でプロットされている
- 表データの行がデータとして、ラベル、連続値、分類値のカラムが存在し、ラベルは予測対象、連続値は回帰モデル、分類値は分類モデルになる
  - ２つの分類値の場合バイナリモデルとかともいう
- 分類モデルでも、その件数を予測したりする場合は回帰でできる

### 回帰と分類

![レストランチップ 男女の分布](/img/2019-08-27-22-24-17.png)

- 線形だとひどい分類予測になってしまいそう
- 非線形の方が、分類には適してそう

## MLの歴史

### 機械学習小史: 線形回帰

- 線形回帰とかNWとか機械学習の歴史
- 線形回帰は惑星の動きやえんどう豆のさやに対する実の大きさを予測するために使われていた
- 線形回帰

### 機械学習小史: パーセプトロン

- 1940年代にフランク ローゼンブラットが 人間の脳のニューロンの計算モデルとして パーセプトロンを考案し 簡単な機能の学習方法を示した
- 何度か壁にあたって忘れ去られたりもしてる

### 機械学習小史: ニューラルネットワーク

- パーセプトロンを複数個つなげたような形の学習モデル

### 機械学習小史: 決定ツリー

- ID3やC4.5
- 貪欲アルゴリズム
- DNNでは実際の各層が結びついている
- データの入れ物を決めてみよう
- yes/noで振り分けていく
- バイナリ分類器のようなもの
- ツリーを再帰的に作るのは、NWの線形分類器ノードの層に似てる
- 実際にツリーの質問は入ってきたデータに対して一般化しておきたい
  
### 機械学習小史: カーネル メソッド

- SVMを含むジャンル
- SVMはマージンをヒンジ関数を使って最大化する
- 次元が多い場合や 予測変数による 応答の予測精度が非常に高い場合 SVMは優れた効果を発揮します 

### 機械学習小史: ランダム フォレスト

- マシンの性能が上がると様々なモデルを組み合わせるようになった　アンサンブル法
- ランダムフォレストはアンサンブル法の一つ

### 機械学習小史: 最新のニューラル ネットワーク

- DNN
- 取り掛かるときに特に大事なこと
  - データ量
  - 実験
  - 一般化

#### ゲレンデの混雑率予想の回答

混雑レベルを予測するには、ゲレンデを滑っている人数を予測できれば良い。

分かっていることは、リフト券を購入した利用客のレベルと、過去の積雪量(誤訳？前のビデオはそれまでの積雪量)。

各利用客レベルとそれまでの積雪量を用いて、ゲレンデに滞在している確率を求める回帰モデルを作成する。

上記モデルに、レベルと、刻々と変化しているそれまでの積雪量を与えて、ゲレンデに滞在している確率を求め、各利用客レベルごとのリフト券購入数と掛け合わせれば、ゲレンデを滑っている人数を予測できそう。

## 最適化について

### 機械学習モデルの定義

- モデルはパラメータとハイパーパラメータを使った数学的関数
- 特徴量と呼ばれる独立変数が変化するとラベルと呼ばれる従属変数も同量だけ変化する
- 2次元領域の直線　y = mx + b
- この考え方は任意の高次元まで拡張できる

### 出生率データセットの概要

- 赤ちゃんが生まれた際に治療が必要になるか予測する必要性
- 特徴量としての候補
  - 赤ちゃんの体重
  - 出生時間
  - 母親の年齢
- うち赤ちゃんの体重と母親の年齢は観測可能なので特徴量として使用できる
- 体重は連続値
- 分布図に線形の先を引いてみてもいまいちに見える

### 損失関数の概要

- モデルの最適なハイパーパラメタを求めるには
- 損失関数を使う
- 平均二乗誤差
- クラス分類の場合は交差エントロピー損失を計算する

### 勾配降下法

- 損失関数の凹凸表面をどのようにして最小値に下っていくのか
  - 等高線のようなもの
- 学習率次第で遅くなったり、最小値にたどり着けなかったりする
- 進む方向は、一番傾斜の深い方向

### 損失関数のトラブルシューティング

- 時間と損失軸の損失曲線を書いてみる
- 反比例の形でないときは問題あり
- 一般に学習率は1より遥かに小さい値
- 